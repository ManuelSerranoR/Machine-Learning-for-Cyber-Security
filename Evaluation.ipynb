{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load all emails and labels in a data frame ###\n",
    "\n",
    "data_train = pd.DataFrame(columns=['X', 'y'])\n",
    "data_test = pd.DataFrame(columns=['X'])\n",
    "\n",
    "count_train = 0\n",
    "count_test = 0\n",
    "\n",
    "#Iterate over the 9 folds to get the training set\n",
    "for i in range(1, 10):\n",
    "    #Load spam emails\n",
    "    for spam_path in glob.glob('lingspam_public/lemm_stop/part'+str(i)+'/spm*.txt'):\n",
    "        with open(spam_path, 'r') as spam_file:\n",
    "            opened_spam_file = spam_file.read().replace('\\n', '')\n",
    "            data_train.loc[count_train] = [opened_spam_file, 1]\n",
    "            count_train += 1\n",
    "            \n",
    "    #Load legitimate emails\n",
    "    for legit_path in glob.glob('lingspam_public/lemm_stop/part'+str(i)+'/[!spm]*.txt'):\n",
    "        with open(legit_path, 'r') as legit_file:\n",
    "            opened_legit_file = legit_file.read().replace('\\n', '')\n",
    "            data_train.loc[count_train] = [opened_legit_file, 0]\n",
    "            count_train += 1\n",
    "\n",
    "#Get the evaluation set under the eval folder\n",
    "for eval_path in sorted(glob.glob('eval/all/*.txt')):\n",
    "    with open(eval_path, 'r') as eval_file:\n",
    "        opened_eval_file = eval_file.read().replace('\\n', '')\n",
    "        data_test.loc[count_test] = [opened_eval_file]\n",
    "        count_test += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultinomialTF_1000 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                               ('feature_selector', SelectKBest(mutual_info_classif, k=1000)), \n",
    "                               ('classifier', MultinomialNB())])\n",
    "MultinomialTF_1000.fit(data_train['X'], data_train['y'].tolist())\n",
    "y_test_pred = MultinomialTF_1000.predict(data_test['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('eval/results.txt','w') \n",
    "for item in y_test_pred:\n",
    "    file.write(\"%s\\n\" % item)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
